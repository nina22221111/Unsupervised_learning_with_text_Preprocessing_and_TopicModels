{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36120096",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re # Package for regular expressions\n",
    "import nltk # Main python package for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us download the dataset from the course repository\n",
    "imdb = pd.read_csv('https://datasciencebocconi.github.io/Data/IMDB_small.csv')\n",
    "imdb.shape # Size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of this dataset\n",
    "imdb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = imdb.iloc[8, 0]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Load the package\n",
    "\n",
    "# Removes the <br /> and other HTML tags\n",
    "def remove_html(data):\n",
    "    data = BeautifulSoup(data)\n",
    "    return data.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_abb(review):\n",
    "    replacements = {\n",
    "       \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"Its\" : \"It is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"wanna\" : \"want to\",\n",
    "        \"We're\" : \"We are\"\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        review = re.sub(r\"{}\".format(key), value, review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf25958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st round of pre-processing\n",
    "def basic_cleaning(review):\n",
    "    review = remove_html\n",
    "    review = remove_abb(review) # Remove abbreviations\n",
    "    return review\n",
    "\n",
    "# 2nd round of Pre-processing\n",
    "def advanced_cleaning(review):\n",
    "  \n",
    "  # Basic cleaning (HTML + symbols)\n",
    "    review = basic_cleaning(review)\n",
    "  \n",
    "  # Normalization\n",
    "    review = review.lower()\n",
    "\n",
    "  # Tokenization\n",
    "    review_tokens = nltk.word_tokenize(review)\n",
    "  \n",
    "  # Special symbols and punctuation\n",
    "    review_tokens = [words for words in review_tokens if words.isalpha()] \n",
    "  \n",
    "  # Filtering\n",
    "    review_tokens = [words for words in review_tokens if words not in stopwords.words('english')]\n",
    "  \n",
    "  # Stemming\n",
    "    review_tokens = [nltk.SnowballStemmer(\"english\").stem(words) for words in review_tokens]\n",
    "  \n",
    "  # Conversion to a single string\n",
    "    review = detokenizer.detokenize(review_tokens)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b227655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original document\n",
    "imdb.iloc[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning\n",
    "basic_cleaning(imdb.iloc[4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After stemming\n",
    "advanced_cleaning(imdb.iloc[4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8914f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could take a while\n",
    "imdb['review_clean'] = imdb['review'].apply(lambda z: basic_cleaning(z))\n",
    "imdb['review_token'] = imdb['review'].apply(lambda z: advanced_cleaning(z))\n",
    "\n",
    "imdb.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaa139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything into a single string\n",
    "words  = ' '.join(imdb['review_token'])\n",
    "# Create a global tokenization\n",
    "tokens = nltk.word_tokenize(words)\n",
    "\n",
    "# Conversion to \"text\"\n",
    "text = nltk.Text(tokens)\n",
    "# Compute the most common words\n",
    "fdist = nltk.FreqDist(text)\n",
    "\n",
    "# Use pandas for organizing and displaying the results\n",
    "df_words = pd.DataFrame(list(fdist.items()), columns = [\"Word\",\"Frequency\"])\n",
    "# Order words from the most frequent\n",
    "df_words = df_words.sort_values(by = \"Frequency\", ascending = False)\n",
    "\n",
    "# Dimension of the dataset\n",
    "df_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b356fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35494cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creation of a TDM with p = 500 words\n",
    "vectorizer = CountVectorizer(max_features = 500)\n",
    "X = vectorizer.fit_transform(imdb['review_token'])\n",
    "word_names = list(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Conversion to dataframe\n",
    "X = pd.DataFrame(X.toarray())\n",
    "# Renaming columns according to words\n",
    "X.columns = word_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58064f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creation of a TDM TF-IDF with p = 500 words\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "X = vectorizer.fit_transform(imdb['review_token'])\n",
    "word_names = list(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Conversion to dataframe\n",
    "X = pd.DataFrame(X.toarray())\n",
    "# Renaming columns according to words\n",
    "X.columns = word_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
